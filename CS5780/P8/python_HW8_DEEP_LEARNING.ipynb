{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--announcements-->\n",
    "<center>\n",
    "    <img src=\"network.png\" width=\"400px\" /></a>\n",
    "</center>\n",
    "\n",
    "<h2>Introduction</h2>\n",
    "\n",
    "In this project, you will implement a some neural networks to solve regression and image classification problems. This project will introduce you to some of the basics of implementing and training these powerful models. For this, we will be exploring more into the deep end with Course staffs' favorite ML framework: PyTorch!\n",
    "\n",
    "For full documentation and details, here is their site https://pytorch.org/. PyTorch is an open source machine learning library based on the Torch library, used for applications such as computer vision and natural language processing, primarily developed by Facebook's AI Research lab. Pytorch is very neat because, as you have seen your assignments, in order to do gradient descent we've had to calculate gradient manually. No more! Pytorch performs automatic differentation, as long as we use their functions in our code.\n",
    "\n",
    "From this project, we will be implementing two neural network models learned in class:\n",
    "* Multi-Layer Perceptrons\n",
    "* Convolutional Neural Networks\n",
    "\n",
    "We will also implement a __training loop__ and __testing loop__ in PyTorch for a handwriting classification task, MNIST.\n",
    "\n",
    "Note: Because we are working with Pytorch functions and Modules, we will be using excusively Pytorch tensors instead of numpy arrays. Let's begin by importing some of the necessary functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "#<GRADED>\n",
    "import numpy as np\n",
    "from numpy.matlib import repmat\n",
    "import sys\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# new torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# new vision-dataset-related torch imports\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# misc imports\n",
    "import random\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Regression on Non-linear Functions</h2>\n",
    "\n",
    "Recall from Project 5, where you were tasked to use gradient descent to find the parameters of a simple linear regression problem. We will re-visit the regression problem, but this time for _nonlinear_ data!\n",
    "\n",
    "Before, we were able to model a simple linear function. Let's see if we can move onto a more complex function. Let's begin by generating some data for the sine function, $y=\\sin (x).$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_nonlinear_data(num_samples=10000):\n",
    "    # generate random x samples for training and test sets\n",
    "    xTr = torch.rand(num_samples, 1) * 2 * np.pi\n",
    "    xTe = torch.rand(int(num_samples * 0.1), 1) * 2 * np.pi\n",
    "    \n",
    "    # gaussian noise for non-linear regression\n",
    "    noise = torch.rand(num_samples, 1) * 0.2\n",
    "    test_noise = torch.rand(int(num_samples * 0.1), 1) * 0.2\n",
    "    \n",
    "    # add noise on the labels for the training set\n",
    "    yTr = torch.sin(xTr) + noise\n",
    "    yTe = torch.sin(xTe) + test_noise\n",
    "    return xTr, xTe, yTr, yTe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nl_xTr, nl_xTe, nl_yTr, nl_yTe = gen_nonlinear_data(num_samples=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the <code>LinearRegressionModel</code> implemented in Project 5. We have copied it below (no need to re-implement it). As a reminder, PyTorch models inherit the class torch.nn.module, and you feed in a batch of $n$ samples as input and you get batch of outputs. Every torch module will implement two functions. __init__ as its constructor, and __forward__ function, which defines what happens when you call the module.\n",
    "\n",
    "For the regression task, we use our familiar mean-squared error loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, ndims):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        \"\"\" pytorch optimizer checks for the properties of the model, and if\n",
    "            the torch.nn.Parameter requires gradient, then the model will update\n",
    "            the parameters automatically.\n",
    "        \"\"\"\n",
    "        self.w = nn.Parameter(torch.randn(ndims, 1), requires_grad=True)\n",
    "        self.b = nn.Parameter(torch.randn(1), requires_grad=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x @ self.w + self.b\n",
    "    \n",
    "def mse_loss(y_pred, y_true):\n",
    "    square_diff = torch.pow((y_pred-y_true), 2)\n",
    "    mean_error = 0.5 * torch.mean(square_diff)\n",
    "    return mean_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a regression training loop in Pytorch, similar to the one from Project 5. We have supplied comments per line to help walk you through what each different part does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_regression_model(xTr, yTr, model, num_epochs, lr=1e-2, print_freq=100, display_loss=True):\n",
    "    \"\"\"Train loop for a neural network model. Please use the Adam optimizer, optim.Adam.\n",
    "    \n",
    "    Input:\n",
    "        xTr:     n x d matrix of regression input data\n",
    "        yTr:     n-dimensional vector of regression labels\n",
    "        model:   nn.Model to be trained\n",
    "        num_epochs: number of epochs to train the model for\n",
    "        lr:      learning rate for the optimizer\n",
    "        print_freq: frequency to display the loss\n",
    "        display_loss: boolean, if we print the loss\n",
    "    \n",
    "    Output:\n",
    "        model:   nn.Module trained model\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)  # create an Adam optimizer for the model parameters\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # need to zero the gradients in the optimizer so we don't\n",
    "        # use the gradients from previous iterations\n",
    "        optimizer.zero_grad()  \n",
    "        pred = model(xTr)  # run the forward pass through the model to compute predictions\n",
    "        loss = mse_loss(pred, yTr)\n",
    "        loss.backward()  # compute the gradient wrt loss\n",
    "        optimizer.step()  # performs a step of gradient descent\n",
    "        if display_loss and (epoch + 1) % print_freq == 0:\n",
    "            print('epoch {} loss {}'.format(epoch+1, loss.item()))\n",
    "    \n",
    "    return model  # return trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by seeing what happens when we run our linear model, <code>LinearRegressionModel</code>, on this data. Run the train loop as before on this new data, and plot the results. Visualize how good of a fit our line is to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ndims = nl_xTr.shape[1]\n",
    "linear_model = LinearRegressionModel(ndims)  # initialize the model\n",
    "linear_model = train_regression_model(nl_xTr, nl_yTr,\n",
    "                                      linear_model, \n",
    "                                      num_epochs=2000, \n",
    "                                      lr=1e-2, \n",
    "                                      print_freq=500)\n",
    "avg_test_error = mse_loss(linear_model(nl_xTe), nl_yTe)  # compute the average test error\n",
    "print('avg test error', avg_test_error.item())\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure()\n",
    "plt.plot(nl_xTr, linear_model(nl_xTr).detach(),linewidth=5.0, color=\"red\", label=\"Prediction Line\")\n",
    "plt.scatter(nl_xTr, nl_yTr, label=\"Train Points\")\n",
    "plt.scatter(nl_xTe, nl_yTe, label=\"Test Points\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needless to say, a linear model doesn't model a sine function very well! Let's instead use our first neural network, a __Multi-Layer Perceptron model__. \n",
    "\n",
    "We need to define a few layers of the <code>MLPNet</code> using Pytorch, and implement the forward pass. For this network, write an MLP that has:\n",
    "* Two fully connected layers: Use the Pytorch implementation, <code>nn.Linear(in_dim, out_dim)</code>. This will be updated automatically by Pytorch in our training loop.\n",
    "* ReLU nonlinearities: Use the Pytorch implementation, <code>nn.functional.relu</code> between each of the linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a Pytorch Multilayer Perceptron (MLP) Model\n",
    "#<GRADED>\n",
    "class MLPNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=1):\n",
    "        super(MLPNet, self).__init__()\n",
    "        \"\"\" pytorch optimizer checks for the properties of the model, and if\n",
    "            the torch.nn.Parameter requires gradient, then the model will update\n",
    "            the parameters automatically.\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        # Initialize the fully connected layers\n",
    "        raise NotImplementedError(\"Your code goes here!\")\n",
    "        self.fc1 = None\n",
    "        \n",
    "        self.fc2 = None\n",
    "            \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Implement the forward pass, with ReLU non-linearities\n",
    "        raise NotImplementedError(\"Your code goes here!\")\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        return x\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the __hidden dimensions__ in an MLP affect the output results? Try out various hidden dimensions using the code provided. We highly recommend for you to play with different number of hidden nodes using the slider tool. Do more or less hidden nodes result in smoother regression outputs? What is the tradeoff?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 2000\n",
    "lr = 1e-3\n",
    "trained_models = {}\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "def update(hdims):\n",
    "    fig.clear()\n",
    "    key = str(hdims)\n",
    "    if key in trained_models:\n",
    "        mlp_model = trained_models[key]\n",
    "    else:\n",
    "        mlp_model = MLPNet(input_dim=1, hidden_dim=hdims, output_dim=1)\n",
    "        mlp_model = train_regression_model(nl_xTr, nl_yTr, mlp_model, num_epochs=num_epochs, lr=lr, display_loss=False)\n",
    "        trained_models[key] = mlp_model\n",
    "    plt.scatter(nl_xTr, nl_yTr, label=\"Train Points\")\n",
    "    plt.scatter(nl_xTe, nl_yTe, label=\"Test Points\")\n",
    "    plt.title('MLP Net')\n",
    "    plt.scatter(nl_xTr, mlp_model(nl_xTr).detach(), color=\"red\", marker='o', label=\"Prediction\")\n",
    "    plt.legend()\n",
    "    fig.canvas.draw()\n",
    "\n",
    "widgets.interact(update, hdims=widgets.IntSlider(value=1, min=1, max=100, step=1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the regression train loop, <code>train_regression_model</code>, with the MLP model on the non-linear data. Use your favorite hidden dimension <code>hdim</code> from above to get the test error below 0.002. Additionally, what learning rates and number of epochs converges for the MLP model? To start, we suggest using <code>num_epochs = 5000</code>, and <code>lr = 1e-3</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hdims = None\n",
    "num_epochs = None\n",
    "lr = None\n",
    "\n",
    "mlp_model = MLPNet(input_dim=1, hidden_dim=hdims, output_dim=1)\n",
    "mlp_model = train_regression_model(nl_xTr, nl_yTr, mlp_model, num_epochs=num_epochs, lr=lr)\n",
    "avg_test_error = mse_loss(mlp_model(nl_xTe), nl_yTe)\n",
    "print('avg test error', avg_test_error.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have trained our first neural network model! Let's predict by passing in input data via <code>mlp_model(x)</code>. \n",
    "\n",
    "Visualize how good of a fit our MLP is to the data, as compared to the linear model via a side-by-side comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8,3))\n",
    "\n",
    "# Plot the visualizations from our MLP Model\n",
    "ax1.scatter(nl_xTr, nl_yTr, label=\"Train Points\")\n",
    "ax1.scatter(nl_xTe, nl_yTe, label=\"Test Points\")\n",
    "ax1.scatter(nl_xTr, mlp_model(nl_xTr).detach(), color=\"red\", marker='o', label=\"Prediction\")\n",
    "ax1.legend()\n",
    "ax1.set_title('MLP Net')\n",
    "\n",
    "# Plot the visualizations from our MLP Model\n",
    "ax2.scatter(nl_xTr, nl_yTr, label=\"Train Points\")\n",
    "ax2.scatter(nl_xTe, nl_yTe, label=\"Test Points\")\n",
    "ax2.plot(nl_xTr, linear_model(nl_xTr).detach(),linewidth=5.0, color=\"red\", label=\"Prediction Line\")\n",
    "ax2.legend()\n",
    "ax2.set_title('Linear Model')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Handwritten Digits Classification</h2>\n",
    "\n",
    "One of the tasks neural network models can solve quite well is classification! We will explore their capabilities on a simple classification task, classifying handwritten digits. Specifically, we will be taking an image of a digit between 0 - 9 and classifying which digit it is.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<center>\n",
    "    <img src=\"320px-MnistExamples.png\" width=\"400px\" /></a>\n",
    "</center>\n",
    "\n",
    "For this task, we will look into the [MNIST dataset](http://yann.lecun.com/exdb/mnist/). The MNIST database consists of black and white handwritten digits, cropped to a fixed size. It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting.\n",
    "\n",
    "Fortunately for us, Pytorch provides an easy implementation to download the cleaned and already prepared data, using a few lines of code. Let us load it and visualize some of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "train_set = dset.MNIST(root=\".\", train=True, transform=trans, download=True)\n",
    "train_subset_indices = np.load(\"train_subset_indices.npy\")  # load a predefined subset for efficiency\n",
    "train_set = torch.utils.data.Subset(train_set, train_subset_indices)\n",
    "test_set = dset.MNIST(root=\".\", train=False, transform=trans, download=True)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img + 0.5     # unnormalize\n",
    "    plt.figure()\n",
    "    plt.imshow(np.transpose(img.numpy(), (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "num_show_img = 4\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images[:num_show_img]))\n",
    "# print labels\n",
    "print('labels are:', ' '.join('%d' % labels[j] for j in range(num_show_img)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the regression train loop (given above in <code>train_regression_model</code>) to train a classification model instead. The key difference is the loss function used is _no longer the MSE loss_. \n",
    "\n",
    "We will instead use the cross entropy loss, which is implemented for us in Pytorch, <code>nn.functional.cross_entropy(predictions, labels)</code>. Implement the new training loop, <code>train_classification_model</code>, and assume that the prediction outputs from the model are in one-hot format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training function\n",
    "#<GRADED>\n",
    "def train_classification_model(train_loader, model, num_epochs, lr=1e-1, print_freq=100):\n",
    "    \"\"\"Train loop for a neural network model. Please use the SGD optimizer, optim.SGD.\n",
    "    \n",
    "    Input:\n",
    "        train_loader:    Data loader for the train set. \n",
    "                         Enumerate through to train with each batch.\n",
    "        model:           nn.Model to be trained\n",
    "        num_epochs:      number of epochs to train the model for\n",
    "        lr:              learning rate for the optimizer\n",
    "        print_freq:      frequency to display the loss\n",
    "    \n",
    "    Output:\n",
    "        model:   nn.Module trained model\n",
    "    \"\"\"\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)  # create an SGD optimizer for the model parameters\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Iterate through the dataloader for each epoch\n",
    "        for batch_idx, (imgs, labels) in enumerate(train_loader):\n",
    "            # imgs (torch.Tensor):    batch of input images\n",
    "            # labels (torch.Tensor):  batch labels corresponding to the inputs\n",
    "            \n",
    "            # Implement the training loop using imgs, labels, and cross entropy loss\n",
    "            raise NotImplementedError(\"Your code goes here!\")\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    return model  # return trained model\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, implement the classification test function. Compute the accuracy of our model over the test set. We say that the predicted label is the digit that the model outputs the _maximum score_, and a correct prediction is one where the maximum score matches the true label. Return the average accuracy over all batches in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test function\n",
    "#<GRADED>\n",
    "def test_classification_model(test_loader, model):\n",
    "    \"\"\"Tests the accuracy of the model.\n",
    "    \n",
    "    Input:\n",
    "        test_loader:      Data loader for the test set. \n",
    "                          Enumerate through to test each example.\n",
    "        model:            nn.Module model being evaluate.\n",
    "        \n",
    "    Output:\n",
    "        accuracy:         Accuracy of the model on the test set.\n",
    "    \"\"\"\n",
    "    # Compute the model accuracy\n",
    "    \n",
    "    accuracy = 0\n",
    "    \n",
    "    for batch_idx, (imgs, labels) in enumerate(test_loader):\n",
    "        raise NotImplementedError(\"Your code goes here!\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return accuracy\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to train an MLP neural network on this classification task! Initialize a <code>MLPNet</code> with the proper input dimension and output dimension. Play around with the hidden dimension to get the accuracy above 95\\%. Additionally, what learning rates and number of epochs converges for the MLP model? To start, we suggest using <code>num_epochs = 40</code> and <code>lr = 1e-2</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train an MLP model on MNSIT\n",
    "hidden_dim = None\n",
    "num_epochs = None\n",
    "lr = None\n",
    "\n",
    "mlp_model = MLPNet(input_dim=28*28, hidden_dim=hidden_dim, output_dim=10)\n",
    "print('the number of parameters', sum(parameter.view(-1).size()[0] for parameter in mlp_model.parameters()))\n",
    "mlp_model = train_classification_model(train_loader, mlp_model, num_epochs=num_epochs, lr=lr)\n",
    "avg_test_acc = test_classification_model(test_loader, mlp_model)\n",
    "print('avg test accuracy', avg_test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Convolutional Neural Networks</h3>\n",
    "\n",
    "In class, we learned about about Convolutional Neural Networks (or CNN for short) that is explicity designed for image inputs. Let's implement and train a CNN on the MNIST dataset!\n",
    "\n",
    "We need to define a few layers of the <code>ConvNet</code> using Pytorch, and implement the forward pass. For this network, write an CNN that has:\n",
    "* Two convolutional layers: Use the Pytorch implementation, <code>nn.Conv2d(in_channels, out_channels)</code>. For each convolutional layer, set <code>kernel_size=3</code>, <code>stride=1</code>, <code>padding=1</code> and <code>bias=False</code>.\n",
    "* ReLU nonlinearities: Use the Pytorch implementation, <code>nn.functional.relu</code> between each of the layers.\n",
    "* Max-Pooling layers: Use the Pytorch implementation, <code>nn.functional.max_pool2d</code> to pool after each of the ReLU. For each max-pooling layer, set <code>kernel_size=2</code>, <code>stride=2</code>.\n",
    "* Last layer a fully connected layer: Use the Pytorch implementation, <code>nn.Linear(in_dim, out_dim)</code>. <code>in_dim</code> depends on the size of input image (MNIST images), which is <code>28 * 28</code> in our case. Make sure to re-shape the input into a batch of vectors to pass it through the last layer using the function: <code>Tensor.view(*shape)</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Pytorch ConvNet Model\n",
    "#<GRADED>\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, input_channels=1, hidden_channels=32, output_dim=1):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        # Initialize the ConvNet layers\n",
    "        raise NotImplementedError(\"Your code goes here!\")\n",
    "        self.conv1 = None\n",
    "        \n",
    "        self.conv2 = None\n",
    "        \n",
    "        self.fc = None\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Implement the forward pass, with ReLU non-linearities and max-pooling\n",
    "        raise NotImplementedError(\"Your code goes here!\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        return x\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the classificiation train loop, this time with the <code>ConvNet</code> implemented above. Try out different hidden channel dimension to obtain an accuracy above 95\\%. (Hint: the channel dimension should not be above 50 for computational efficiency.)\n",
    "\n",
    "Once again, what learning rates and number of epochs converges for the CNN model? To start, we suggest using <code>num_epochs = 40</code> and <code>lr = 1e-3</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train a convnet model on MNIST\n",
    "hidden_channels = None\n",
    "num_epochs = None\n",
    "lr = None\n",
    "\n",
    "conv_model = ConvNet(input_channels=1, hidden_channels=hidden_channels, output_dim=10)\n",
    "print('the number of parameters:', sum(parameter.view(-1).size()[0] for parameter in conv_model.parameters()))\n",
    "conv_model = train_classification_model(train_loader, conv_model, num_epochs=num_epochs, lr=lr)\n",
    "avg_test_acc = test_classification_model(test_loader, conv_model)\n",
    "print('avg test accuracy', avg_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# Make a forward pass to get predictions of the MLP model\n",
    "mlp_scores = mlp_model(images)\n",
    "_, mlp_preds = torch.max(mlp_scores.data, 1)\n",
    "\n",
    "# Make a forward pass to get predictions of the ConvNet model\n",
    "conv_scores = conv_model(images)\n",
    "_, conv_preds = torch.max(conv_scores.data, 1)\n",
    "\n",
    "show_img_idx = np.random.randint(images.shape[0], size=7)\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images[show_img_idx]))\n",
    "# print labels\n",
    "print('labels are:', ' '.join('%d' % labels[j] for j in show_img_idx))\n",
    "# print predictions\n",
    "print('MLP predictions are:', ' '.join('%d' % mlp_preds[j] for j in show_img_idx))\n",
    "print('CNN predictions are:', ' '.join('%d' % conv_preds[j] for j in show_img_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Why Do CNN's Work?</h3>\n",
    "\n",
    "Let's try to build some intuition behind why Convolution Neural Networks work well for image inputs.\n",
    "\n",
    "For a toy experiment, let's shuffle the pixels for each of the input images, _but only in a set order_. Would we expect to see ConvNet's still working better than MLP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the shuffled dataset\n",
    "torch.manual_seed(0)\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "pixel_shuffle_train_set = dset.MNIST(root=\".\", train=True, transform=trans, download=True)\n",
    "train_subset_indices = np.load(\"train_subset_indices.npy\")  # load a subset for efficiency\n",
    "pixel_shuffle_train_set = torch.utils.data.Subset(pixel_shuffle_train_set, train_subset_indices)\n",
    "pixel_shuffle_test_set = dset.MNIST(root=\".\", train=False, transform=trans, download=True)\n",
    "\n",
    "randperm = torch.randperm(28 * 28)\n",
    "pixel_shuffle_train_set.dataset.data = pixel_shuffle_train_set.dataset.data.view(-1, 28 * 28)[:, randperm].view(-1, 28, 28)\n",
    "pixel_shuffle_test_set.data = pixel_shuffle_test_set.data.view(-1, 28 * 28)[:, randperm].view(-1, 28, 28)\n",
    "\n",
    "batch_size = 64\n",
    "pixel_shuffle_train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=pixel_shuffle_train_set,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True)\n",
    "pixel_shuffle_test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=pixel_shuffle_test_set,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataiter = iter(pixel_shuffle_train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "num_show_img = 4\n",
    "# show shuffled images\n",
    "imshow(torchvision.utils.make_grid(images[:num_show_img]))\n",
    "# print labels\n",
    "print('labels are:', ' '.join('%d' % labels[j] for j in range(num_show_img)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like kind of a mess! But let's train an MLP model anyways, as before, and observe the new test accuracy on this shuffled-pixel dataset. __Use the same hyperparameters as you did before__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train an MLP model on pixel-shuffled MNSIT\n",
    "hidden_dim = None\n",
    "num_epochs = None\n",
    "lr = None\n",
    "\n",
    "mlp_model = MLPNet(input_dim=28*28, hidden_dim=hidden_dim, output_dim=10)\n",
    "print('the number of parameters:', sum(parameter.view(-1).size()[0] for parameter in mlp_model.parameters()))\n",
    "mlp_model = train_classification_model(pixel_shuffle_train_loader, mlp_model, num_epochs=num_epochs, lr=lr)\n",
    "avg_test_acc = test_classification_model(pixel_shuffle_test_loader, mlp_model)\n",
    "print('avg test accuracy', avg_test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, train a CNN model, as before, and observe the new test accuracy on this shuffled-pixel dataset. __Use the same hyperparameters as you did before__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train a ConvNet model on pixel-shuffled MNSIT\n",
    "hidden_channels = None\n",
    "num_epochs = None\n",
    "lr = None\n",
    "\n",
    "conv_model = ConvNet(input_channels=1, hidden_channels=hidden_channels, output_dim=10)\n",
    "print('the number of parameters:', sum(parameter.view(-1).size()[0] for parameter in conv_model.parameters()))\n",
    "conv_model = train_classification_model(pixel_shuffle_train_loader, conv_model, num_epochs=num_epochs, lr=lr)\n",
    "avg_test_acc = test_classification_model(pixel_shuffle_test_loader, conv_model)\n",
    "print('avg test accuracy', avg_test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the accuracy is now _slightly lower than_ that of MLP models! This shows that CNN's utilize the stucture of the image (i.e., pixel proximity) to make their predictions. With the pixel orders shuffled, it is no longer as good of a model.\n",
    "\n",
    "Let us visualize some of the prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataiter = iter(pixel_shuffle_test_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# Make a forward pass to get predictions of the MLP model\n",
    "mlp_scores = mlp_model(images)\n",
    "_, mlp_preds = torch.max(mlp_scores.data, 1)\n",
    "\n",
    "# Make a forward pass to get predictions of the ConvNet model\n",
    "conv_scores = conv_model(images)\n",
    "_, conv_preds = torch.max(conv_scores.data, 1)\n",
    "\n",
    "show_img_idx = np.random.randint(images.shape[0], size=7)\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images[show_img_idx]))\n",
    "# print labels\n",
    "print('labels are:', ' '.join('%d' % labels[j] for j in show_img_idx))\n",
    "# print predictions\n",
    "print('MLP predictions are:', ' '.join('%d' % mlp_preds[j] for j in show_img_idx))\n",
    "print('CNN predictions are:', ' '.join('%d' % conv_preds[j] for j in show_img_idx))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
